{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23730aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23730aaa",
        "outputId": "03d4f057-e5a7-4603-9a30-b62b387e8802"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/takatakiyugo/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, bisect\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import os\n",
        "# Download Gutenberg corpus\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# 文書読み込み\n",
        "fileids = gutenberg.fileids()\n",
        "docs = [gutenberg.raw(fid) for fid in fileids]\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85ac3075",
      "metadata": {
        "id": "85ac3075"
      },
      "outputs": [],
      "source": [
        "def tokenize_en(text):\n",
        "    \"\"\"英語テキストを小文字化して単語リストに分割する簡易トークナイザ\"\"\"\n",
        "    return re.findall(r\"[a-zA-Z]+\", text.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66702841",
      "metadata": {
        "id": "66702841"
      },
      "outputs": [],
      "source": [
        "def show_postings(label, postings, k=60):\n",
        "    print(f\"{label}: df={len(postings)} head={postings[:k]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d010c7",
      "metadata": {
        "id": "97d010c7"
      },
      "source": [
        "### 文書のチャンキング\n",
        "500ワードごとに分割して文書数を増やす。RAGなどにも利用可能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d987315",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d987315",
        "outputId": "1771e8cf-6dce-4c16-8bac-b08bc247f682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original docs: 18\n",
            "Chunked docs: 4279\n"
          ]
        }
      ],
      "source": [
        "def chunk_documents(docs, chunk_size=500):\n",
        "    chunks = []\n",
        "    for doc in docs:\n",
        "        words = tokenize_en(doc)\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i+chunk_size])\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "chunked_docs = chunk_documents(docs, 500)\n",
        "print(\"Original docs:\", len(docs))\n",
        "print(\"Chunked docs:\", len(chunked_docs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a25649a",
      "metadata": {
        "id": "9a25649a"
      },
      "source": [
        "### Grep-like Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "895b16f2",
      "metadata": {
        "id": "895b16f2"
      },
      "source": [
        "### 文書のチャンキング\n",
        "EDAは作品単位でしたが、検索以降は500ワードごとに分割した文書を使います。\n",
        "これにより文書数が増え、RAG実習にも適した形になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "78150c1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78150c1f",
        "outputId": "c82236d2-fc75-4ca2-f83d-8d6dcd7f14f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original docs: 18\n",
            "Chunked docs: 4279\n",
            "Total tokens after chunking: 2136080\n",
            "Unique vocab after chunking: 41509\n"
          ]
        }
      ],
      "source": [
        "def chunk_documents(docs, chunk_size=500):\n",
        "    chunks = []\n",
        "    for doc in docs:\n",
        "        words = tokenize_en(doc)\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i+chunk_size])\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "chunked_docs = chunk_documents(docs, 500)\n",
        "print(\"Original docs:\", len(docs))\n",
        "print(\"Chunked docs:\", len(chunked_docs))\n",
        "\n",
        "# チャンキング後の基礎統計\n",
        "total_tokens = sum(len(tokenize_en(doc)) for doc in chunked_docs)\n",
        "unique_tokens = len(set(w for doc in chunked_docs for w in tokenize_en(doc)))\n",
        "\n",
        "print(\"Total tokens after chunking:\", total_tokens)\n",
        "print(\"Unique vocab after chunking:\", unique_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b24895a3",
      "metadata": {
        "id": "b24895a3"
      },
      "outputs": [],
      "source": [
        "def grep_search_raw(term, docs):\n",
        "    result = []\n",
        "    for i, d in enumerate(docs):\n",
        "        if term in d.lower():\n",
        "            result.append(i)\n",
        "    print(f\"[Raw grep] Checked {len(docs)} docs in total\")\n",
        "    return result\n",
        "\n",
        "def grep_search_token(term, docs):\n",
        "    result = []\n",
        "    for i, d in enumerate(docs):\n",
        "        tokens = tokenize_en(d)\n",
        "        if term in tokens:\n",
        "            result.append(i)\n",
        "    print(f\"[Token grep] Checked {len(docs)} docs in total\")\n",
        "    return result\n",
        "\n",
        "def grep_and_search(term1, term2, docs):\n",
        "    res1 = set(grep_search_token(term1, docs))\n",
        "    res2 = set(grep_search_token(term2, docs))\n",
        "    return sorted(list(res1 & res2)) # 2つの結果を集合演算しているだけ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3609eeb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3609eeb",
        "outputId": "8944e979-2fb2-40e0-8417-8c8a64ebf78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Raw grep] Checked 4279 docs in total\n",
            "Raw grep 'god': df=1752 head=[10, 11, 12, 13, 14, 15, 16, 30, 34, 35, 37, 39, 41, 43, 44, 70, 71, 72, 74, 82, 92, 93, 101, 115, 118, 120, 121, 137, 138, 139, 155, 191, 220, 221, 261, 263, 264, 270, 274, 282, 286, 301, 314, 316, 317, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 662, 666, 692]\n",
            "[Token grep] Checked 4279 docs in total\n",
            "Token grep 'god': df=1609 head=[101, 261, 263, 264, 270, 274, 286, 314, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 666, 692, 693, 697, 699, 703, 711, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 751, 752, 753, 754, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 770, 771, 772, 773, 774, 776, 777]\n",
            "[Token grep] Checked 4279 docs in total\n",
            "[Token grep] Checked 4279 docs in total\n",
            "Token grep AND 'love' AND 'god': df=231 head=[264, 480, 481, 488, 666, 703, 770, 776, 841, 843, 910, 911, 1002, 1004, 1005, 1010, 1011, 1012, 1015, 1023, 1041, 1042, 1081, 1084, 1095, 1114, 1245, 1382, 1427, 1496, 1497, 1502, 1511, 1517, 1533, 1534, 1549, 1560, 1563, 1568, 1570, 1579, 1613, 1619, 1622, 1690, 1695, 1700, 1702, 1704, 1709, 1747, 1813, 1827, 1845, 1873, 1889, 1898, 1899, 1903]\n"
          ]
        }
      ],
      "source": [
        "# 出力（df + 先頭30件）\n",
        "res_raw_god = grep_search_raw(\"god\", chunked_docs)\n",
        "show_postings(\"Raw grep 'god'\", res_raw_god)\n",
        "\n",
        "res_tok_god = grep_search_token(\"god\", chunked_docs)\n",
        "show_postings(\"Token grep 'god'\", res_tok_god)\n",
        "\n",
        "res_tok_and = grep_and_search(\"love\", \"god\", chunked_docs)\n",
        "show_postings(\"Token grep AND 'love' AND 'god'\", res_tok_and)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z1LrFOQzhx6b",
      "metadata": {
        "id": "z1LrFOQzhx6b"
      },
      "source": [
        "### raw grep\n",
        "\n",
        "### token grep\n",
        "\n",
        "\n",
        "df = document frequency : 何個の文書にこの単語が入っているか\n",
        "\n",
        "dfがraw grepとtoken grepで違う\n",
        "\n",
        "raw grepではgodivaなどでマッチする\n",
        "\n",
        "token grepではそれがない"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8323ef86",
      "metadata": {
        "id": "8323ef86"
      },
      "source": [
        "## 転置インデックス (array+bisect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "da61eb97",
      "metadata": {
        "id": "da61eb97"
      },
      "outputs": [],
      "source": [
        "class InvertedIndexArray:\n",
        "    def __init__(self):\n",
        "        self.vocab = []       # ソート済み語彙リスト\n",
        "        self.postings = {}    # term -> posting list\n",
        "\n",
        "    def build(self, docs):\n",
        "        vocab_set = set()\n",
        "        postings = {}\n",
        "        for doc_id, doc in enumerate(docs):\n",
        "            for w in set(tokenize_en(doc)):\n",
        "                vocab_set.add(w)\n",
        "                postings.setdefault(w, []).append(doc_id)\n",
        "        self.vocab = sorted(vocab_set)\n",
        "        self.postings = {t: sorted(lst) for t, lst in postings.items()}\n",
        "\n",
        "    # --- 自前の二分探索 ---\n",
        "    def binary_search(self, arr, target):\n",
        "        left, right = 0, len(arr) - 1\n",
        "        while left <= right:\n",
        "            mid = (left + right) // 2\n",
        "            if arr[mid] == target:\n",
        "                return mid\n",
        "            elif arr[mid] < target:\n",
        "                left = mid + 1\n",
        "            else:\n",
        "                right = mid - 1\n",
        "        return -1  # not found\n",
        "\n",
        "    def search(self, term):\n",
        "        i = self.binary_search(self.vocab, term)\n",
        "        if i != -1:\n",
        "            return self.postings[self.vocab[i]]\n",
        "        return []\n",
        "\n",
        "    # --- AND検索 (1) set方式 ---\n",
        "    def and_search_set(self, t1, t2):\n",
        "        \"\"\"集合演算ベース: 簡単・短いが、メモリ確保とO(n+m)コスト\"\"\"\n",
        "        return sorted(set(self.search(t1)) & set(self.search(t2)))\n",
        "\n",
        "    # --- AND検索 (2) マージ方式 ---\n",
        "    def and_search_merge(self, t1, t2):\n",
        "        \"\"\"posting listがソート済みであることを利用した2ポインタ法\"\"\"\n",
        "        p1 = self.search(t1)\n",
        "        p2 = self.search(t2)\n",
        "        i, j = 0, 0\n",
        "        result = []\n",
        "        while i < len(p1) and j < len(p2):\n",
        "            if p1[i] == p2[j]:\n",
        "                result.append(p1[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif p1[i] < p2[j]:\n",
        "                i += 1\n",
        "            else:\n",
        "                j += 1\n",
        "        return result\n",
        "\n",
        "    def and_search_skip(p1, p2):\n",
        "        n1, n2 = len(p1), len(p2)\n",
        "        skip1 = int(math.sqrt(n1)) or 1\n",
        "        skip2 = int(math.sqrt(n2)) or 1\n",
        "\n",
        "        i, j = 0, 0\n",
        "        result = []\n",
        "        while i < n1 and j < n2:\n",
        "            if p1[i] == p2[j]:\n",
        "                result.append(p1[i]); i += 1; j += 1\n",
        "            elif p1[i] < p2[j]:\n",
        "                if (i + skip1 < n1) and (p1[i + skip1] <= p2[j]):\n",
        "                    i += skip1\n",
        "                else:\n",
        "                    i += 1\n",
        "            else:\n",
        "                if (j + skip2 < n2) and (p2[j + skip2] <= p1[i]):\n",
        "                    j += skip2\n",
        "                else:\n",
        "                    j += 1\n",
        "        return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "69707592",
      "metadata": {
        "id": "69707592"
      },
      "outputs": [],
      "source": [
        "inv_arr=InvertedIndexArray(); inv_arr.build(chunked_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c273b710",
      "metadata": {
        "id": "c273b710"
      },
      "source": [
        "| 方法                                | メリット                                                                 | デメリット                                                                 |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------------------- |\n",
        "| **set方式** (`and_search_set`)        | - 実装が簡単で数行<br>- Python組込みの `set` に任せられる                                | - posting list を集合に変換するため順序情報を失う<br>- 内部で余計なメモリコピーとハッシュ計算が発生<br>- 大規模IRではほぼ使われない |\n",
        "| **マージ方式** (`and_search_merge`)   | - posting list がソート済みであることを活用<br>- 計算量は O(n+m) で効率的<br>- 追加メモリはほぼ不要 | - 実装がやや長い<br>- posting list がソートされていないと使えない                                |\n",
        "| **Skip List** (`and_search_skip`)     | - 長大な posting list の比較回数を減らせる<br>- 特に交差が少ない場合に高速<br>- Lucene など実システムでも採用 | - 実装が複雑<br>- skip pointer の設計（間隔など）に工夫が必要                                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K7OBOn3SkBJf",
      "metadata": {
        "id": "K7OBOn3SkBJf"
      },
      "source": [
        "skip listについては第1回授業資料を確認"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQABPupQmVaI",
      "metadata": {
        "id": "KQABPupQmVaI"
      },
      "source": [
        "googleなどはFSTを使っていると言われている"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf33ba80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf33ba80",
        "outputId": "aee35050-ceee-4f69-ed8e-19674a2fc42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Array+bisect 'love': df=649 head=[6, 7, 18, 23, 24, 25, 28, 30, 36, 37, 38, 40, 42, 46, 48, 49, 50, 54, 55, 56, 57, 58, 60, 61, 62, 67, 72, 73, 74, 81, 83, 85, 88, 89, 90, 93, 98, 109, 117, 120, 132, 133, 134, 135, 136, 138, 140, 142, 144, 148, 149, 170, 172, 173, 174, 178, 191, 207, 208, 215]\n",
            "Array+bisect 'god': df=1609 head=[101, 261, 263, 264, 270, 274, 286, 314, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 666, 692, 693, 697, 699, 703, 711, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 751, 752, 753, 754, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 770, 771, 772, 773, 774, 776, 777]\n",
            "Array+bisect AND 'love' & 'god': df=231 head=[264, 480, 481, 488, 666, 703, 770, 776, 841, 843, 910, 911, 1002, 1004, 1005, 1010, 1011, 1012, 1015, 1023, 1041, 1042, 1081, 1084, 1095, 1114, 1245, 1382, 1427, 1496, 1497, 1502, 1511, 1517, 1533, 1534, 1549, 1560, 1563, 1568, 1570, 1579, 1613, 1619, 1622, 1690, 1695, 1700, 1702, 1704, 1709, 1747, 1813, 1827, 1845, 1873, 1889, 1898, 1899, 1903]\n"
          ]
        }
      ],
      "source": [
        "show_postings(\"Array+bisect 'love'\", inv_arr.search(\"love\"))\n",
        "show_postings(\"Array+bisect 'god'\", inv_arr.search(\"god\"))\n",
        "show_postings(\"Array+bisect AND 'love' & 'god'\", inv_arr.and_search_merge(\"love\",\"god\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gv_sPHe9jE88",
      "metadata": {
        "id": "gv_sPHe9jE88"
      },
      "source": [
        "token grepのdfと同じことから確認できる。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93b4e1b",
      "metadata": {
        "id": "a93b4e1b"
      },
      "source": [
        "### Patricia Tree Implementation (Explained)\n",
        "Patricia Tree is a compressed trie structure. We merge common prefixes to reduce node count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "89096678",
      "metadata": {
        "id": "89096678"
      },
      "outputs": [],
      "source": [
        "class PatriciaNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_word = False\n",
        "        self.postings = []\n",
        "\n",
        "class PatriciaTree:\n",
        "    def __init__(self):\n",
        "        self.root = PatriciaNode()\n",
        "\n",
        "    def insert(self, word, doc_id):\n",
        "        node = self.root\n",
        "        w = word\n",
        "        while w:\n",
        "            for k in list(node.children.keys()):\n",
        "                prefix = os.path.commonprefix([k, w])\n",
        "                if prefix:\n",
        "                     if prefix != k:\n",
        "                        child = node.children.pop(k)\n",
        "                        new_node = PatriciaNode()\n",
        "                        new_node.children[k[len(prefix):]] = child\n",
        "                        node.children[prefix] = new_node\n",
        "                        node = new_node\n",
        "                     else:\n",
        "                        node = node.children[k]\n",
        "                     w = w[len(prefix):]\n",
        "                     break\n",
        "            else:\n",
        "                node.children[w] = PatriciaNode()\n",
        "                node = node.children[w]\n",
        "                w = \"\"\n",
        "        node.is_word = True\n",
        "        if doc_id not in node.postings:\n",
        "            node.postings.append(doc_id)\n",
        "\n",
        "    def search(self, word):\n",
        "        node = self.root\n",
        "        w = word\n",
        "        while w:\n",
        "            for k, child in node.children.items():\n",
        "                if w.startswith(k):\n",
        "                    w = w[len(k):]\n",
        "                    node = child\n",
        "                    break\n",
        "            else:\n",
        "                return []\n",
        "        return sorted(node.postings) if node.is_word else []\n",
        "\n",
        "    def starts_with(self, prefix):\n",
        "        node = self.root\n",
        "        w = prefix\n",
        "        while w:\n",
        "            for k, child in node.children.items():\n",
        "                if w.startswith(k):\n",
        "                    w = w[len(k):]\n",
        "                    node = child\n",
        "                    break\n",
        "                elif k.startswith(w):\n",
        "                    return True\n",
        "            else:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "# 全 chunked_docs から構築\n",
        "tree = PatriciaTree()\n",
        "for doc_id, doc in enumerate(chunked_docs):\n",
        "    for w in set(tokenize_en(doc)):\n",
        "        tree.insert(w, doc_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9rPzVToMm-za",
      "metadata": {
        "id": "9rPzVToMm-za"
      },
      "source": [
        "トライ木を作るが、例えば、god - iva\n",
        "のようにノードを圧縮する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a07098ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07098ea",
        "outputId": "4807b1e0-254f-47c2-c494-9ea7f4fb46d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia search 'love': df=649 head=[6, 7, 18, 23, 24, 25, 28, 30, 36, 37, 38, 40, 42, 46, 48, 49, 50, 54, 55, 56, 57, 58, 60, 61, 62, 67, 72, 73, 74, 81, 83, 85, 88, 89, 90, 93, 98, 109, 117, 120, 132, 133, 134, 135, 136, 138, 140, 142, 144, 148, 149, 170, 172, 173, 174, 178, 191, 207, 208, 215]\n",
            "Patricia search 'god': df=1609 head=[101, 261, 263, 264, 270, 274, 286, 314, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 666, 692, 693, 697, 699, 703, 711, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 751, 752, 753, 754, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 770, 771, 772, 773, 774, 776, 777]\n",
            "Patricia AND 'love' & 'god': df=231 head=[264, 480, 481, 488, 666, 703, 770, 776, 841, 843, 910, 911, 1002, 1004, 1005, 1010, 1011, 1012, 1015, 1023, 1041, 1042, 1081, 1084, 1095, 1114, 1245, 1382, 1427, 1496, 1497, 1502, 1511, 1517, 1533, 1534, 1549, 1560, 1563, 1568, 1570, 1579, 1613, 1619, 1622, 1690, 1695, 1700, 1702, 1704, 1709, 1747, 1813, 1827, 1845, 1873, 1889, 1898, 1899, 1903]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 出力（df + head30）\n",
        "show_postings(\"Patricia search 'love'\", tree.search(\"love\"))\n",
        "show_postings(\"Patricia search 'god'\", tree.search(\"god\"))\n",
        "show_postings(\"Patricia AND 'love' & 'god'\", sorted(set(tree.search(\"love\")) & set(tree.search(\"god\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d10be0",
      "metadata": {
        "id": "54d10be0"
      },
      "source": [
        "\n",
        "### 結果の一貫性チェック（chunk 単位）  \n",
        "`grep（単語ベース）` / `Patricia`  の **posting list（チャンクIDの集合）が一致** していることを確認します。  \n",
        "長大な配列は見づらいので、件数（df）と先頭数件だけを表示します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c06c16a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06c16a3",
        "outputId": "51663fe1-40c9-45d1-be13-6905c6cbaca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Token grep] Checked 4279 docs in total\n",
            "=== term: 'love' ===\n",
            "grep(token): df=649 head=[6, 7, 18, 23, 24, 25, 28, 30, 36, 37, 38, 40, 42, 46, 48, 49, 50, 54, 55, 56, 57, 58, 60, 61, 62, 67, 72, 73, 74, 81]\n",
            "patricia   : df=649 head=[6, 7, 18, 23, 24, 25, 28, 30, 36, 37, 38, 40, 42, 46, 48, 49, 50, 54, 55, 56, 57, 58, 60, 61, 62, 67, 72, 73, 74, 81]\n",
            "Sets equal: True\n",
            "\n",
            "[Token grep] Checked 4279 docs in total\n",
            "=== term: 'god' ===\n",
            "grep(token): df=1609 head=[101, 261, 263, 264, 270, 274, 286, 314, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 666, 692, 693, 697, 699, 703, 711, 735, 736, 737]\n",
            "patricia   : df=1609 head=[101, 261, 263, 264, 270, 274, 286, 314, 325, 369, 396, 397, 400, 458, 480, 481, 488, 595, 600, 659, 666, 692, 693, 697, 699, 703, 711, 735, 736, 737]\n",
            "Sets equal: True\n",
            "\n",
            "[Token grep] Checked 4279 docs in total\n",
            "[Token grep] Checked 4279 docs in total\n",
            "=== AND: 'love' AND 'god' ===\n",
            "grep(token): df=231 head=[264, 480, 481, 488, 666, 703, 770, 776, 841, 843, 910, 911, 1002, 1004, 1005, 1010, 1011, 1012, 1015, 1023, 1041, 1042, 1081, 1084, 1095, 1114, 1245, 1382, 1427, 1496]\n",
            "patricia   : df=231 head=[264, 480, 481, 488, 666, 703, 770, 776, 841, 843, 910, 911, 1002, 1004, 1005, 1010, 1011, 1012, 1015, 1023, 1041, 1042, 1081, 1084, 1095, 1114, 1245, 1382, 1427, 1496]\n",
            "Sets equal: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def summarize(name, lst, k=30):\n",
        "    print(f\"{name}: df={len(lst)} head={lst[:k]}\")\n",
        "\n",
        "def compare_across(term):\n",
        "    g = grep_search_token(term, chunked_docs)\n",
        "    p = tree.search(term)\n",
        "    print(f\"=== term: '{term}' ===\")\n",
        "    summarize(\"grep(token)\", g)\n",
        "    summarize(\"patricia   \", p)\n",
        "    print(\"Sets equal:\", set(g) == set(p) )\n",
        "    print()\n",
        "\n",
        "def compare_and(term1, term2):\n",
        "    g = sorted(set(grep_search_token(term1, chunked_docs)) & set(grep_search_token(term2, chunked_docs)))\n",
        "    p = sorted(set(tree.search(term1)) & set(tree.search(term2)))\n",
        "    print(f\"=== AND: '{term1}' AND '{term2}' ===\")\n",
        "    summarize(\"grep(token)\", g)\n",
        "    summarize(\"patricia   \", p)\n",
        "    print(\"Sets equal:\", set(g) == set(p) )\n",
        "    print()\n",
        "\n",
        "compare_across(\"love\")\n",
        "compare_across(\"god\")\n",
        "compare_and(\"love\", \"god\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af32552",
      "metadata": {
        "id": "9af32552"
      },
      "source": [
        "\n",
        "## 検索手法のまとめ（英語コーパス・チャンク単位）\n",
        "\n",
        "- **Raw grep（部分一致）**  \n",
        "  文字列の部分一致でヒット判定。単語境界を無視するためノイズが混ざりやすい。計算量は文書数に対して **O(N)**。\n",
        "\n",
        "- **Token grep（単語一致）**  \n",
        "  簡易トークナイザで分割し、単語集合に対して線形探索。結果は正確だが依然 **O(N)**。\n",
        "\n",
        "- **転置インデックス（array + binary search）**  \n",
        "  `term → posting list(docIDの昇順配列)` を構築。語彙はソートして二分探索（**O(log |V|)**）、\n",
        "  posting list は二分探索や交差（AND）で利用（**O(log |P|)** または **O(|P1|+|P2|)**）。\n",
        "  検索コストが小さく、実用的。\n",
        "\n",
        "- **Patricia木（圧縮Trie, 接頭辞検索のデモ）**  \n",
        "  文字列を辺ラベル単位で前方圧縮。**接頭辞検索**が高速（補完・辞書用途に有効）。\n",
        "  本ノートでは構造理解を主目的とし、posting の正確性は **転置インデックス／** を基準にする。\n",
        "\n",
        "\n",
        "> 以降の実装演習（圧縮・TF-IDF・RAGなど）は、転置インデックスを基盤として進めます。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
